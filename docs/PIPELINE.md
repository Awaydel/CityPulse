# ğŸ”„ EcoSense Analytics: ETL Pipeline Architecture

**ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: ĞºĞ°Ğº ÑƒÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¾ Ğ¸ ĞºĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¾Ğ·Ğ´ÑƒÑ…Ğ°**

---

## ğŸ“‘ Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ

1. [ĞĞ±Ñ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°](#Ğ¾Ğ±Ñ‰Ğ°Ñ-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°)
2. [Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ: Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸](#Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸)
3. [Extract: Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…](#extract-ÑĞ±Ğ¾Ñ€-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
4. [Transform: ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ](#transform-Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ)
5. [Validate (DQ): ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°](#validate-dq-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ-ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°)
6. [Load: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ² Ğ‘Ğ”](#load-Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°-Ğ²-Ğ±Ğ´)
7. [Storage: Ğ’Ğ¸Ñ‚Ñ€Ğ¸Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…](#storage-Ğ²Ğ¸Ñ‚Ñ€Ğ¸Ğ½Ğ°-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
8. [API: REST endpoints](#api-rest-endpoints)
9. [UI: Ğ¤Ñ€Ğ¾Ğ½Ñ‚ĞµĞ½Ğ´](#ui-Ñ„Ñ€Ğ¾Ğ½Ñ‚ĞµĞ½Ğ´)
10. [Scheduling & Monitoring](#scheduling--monitoring)
11. [Error Handling & Retry Logic](#error-handling--retry-logic)

---

## ğŸ—ï¸ ĞĞ±Ñ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES (EXTERNAL)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Open-Meteo Weather API  â”‚  Open-Meteo Air Quality API                      â”‚
â”‚  (Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°, Ğ²Ğ»Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ,â”‚  (PM2.5, PM10)                                   â”‚
â”‚   ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ‚Ñ€Ğ°)        â”‚                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚  EXTRACT (Fetch historical + recent data)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ETL PIPELINE (Python)                               â”‚
â”‚                       services/etl.py                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ EXTRACT PHASE                                                       â”‚   â”‚
â”‚  â”‚ â€¢ fetch_open_meteo_data(lat, lng)                                   â”‚   â”‚
â”‚  â”‚ â€¢ Historical: past_days=7 + forecast_days=1                         â”‚   â”‚
â”‚  â”‚ â€¢ Hourly granularity                                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ TRANSFORM PHASE                                                     â”‚   â”‚
â”‚  â”‚ â€¢ transform_data(w_data, aq_data)                                   â”‚   â”‚
â”‚  â”‚ â€¢ Merge weather + air quality on timestamp                          â”‚   â”‚
â”‚  â”‚ â€¢ Remove negative PM values (outliers)                              â”‚   â”‚
â”‚  â”‚ â€¢ train_and_predict_ml(df)                                          â”‚   â”‚
â”‚  â”‚ â€¢ Linear Regression: predict PM2.5 from (temp, wind, humidity)      â”‚   â”‚
â”‚  â”‚ â€¢ Add predicted_pm25 column                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ VALIDATE PHASE (Data Quality Checks)                                â”‚   â”‚
â”‚  â”‚ â€¢ Check data completeness (nulls, NaNs)                             â”‚   â”‚
â”‚  â”‚ â€¢ Check value ranges (PM >= 0, temp realistic)                      â”‚   â”‚
â”‚  â”‚ â€¢ Check timestamp monotonicity                                      â”‚   â”‚
â”‚  â”‚ â€¢ Detect anomalies (sudden spikes > 50% change)                     â”‚   â”‚
â”‚  â”‚ â€¢ Log validation results                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ LOAD PHASE                                                          â”‚   â”‚
â”‚  â”‚ â€¢ load_to_db(conn, df, city_name, lat, lng, country)               â”‚   â”‚
â”‚  â”‚ â€¢ Upsert dim_city (ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ²)                              â”‚   â”‚
â”‚  â”‚ â€¢ Batch insert into fact_weather (execute_values)                   â”‚   â”‚
â”‚  â”‚ â€¢ Batch insert into fact_air_quality                                â”‚   â”‚
â”‚  â”‚ â€¢ Handle conflicts: ON CONFLICT DO UPDATE                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATABASE LAYER (PostgreSQL)                            â”‚
â”‚                                                                             â”‚
â”‚  Dimension Tables:           Fact Tables:          Aggregates:             â”‚
â”‚  â€¢ dim_city                  â€¢ fact_weather        â€¢ dm_dashboard_analytics â”‚
â”‚    - city_id (PK)              - weather_id (PK)    (SQL VIEW)             â”‚
â”‚    - name                       - city_id (FK)     - Joins facts+dims      â”‚
â”‚    - country_code              - timestamp         - Ready for API         â”‚
â”‚    - latitude                  - temperature                               â”‚
â”‚    - longitude                 - humidity                                  â”‚
â”‚                                - wind_speed       Indexes:                 â”‚
â”‚                              - created_at         â€¢ idx_weather_time       â”‚
â”‚                              - UNIQUE(city_id,    â€¢ idx_aq_time           â”‚
â”‚                                timestamp)                                  â”‚
â”‚                                                                            â”‚
â”‚                            â€¢ fact_air_quality                              â”‚
â”‚                              - aq_id (PK)                                  â”‚
â”‚                              - city_id (FK)                                â”‚
â”‚                              - timestamp                                   â”‚
â”‚                              - pm10                                        â”‚
â”‚                              - pm25                                        â”‚
â”‚                              - predicted_pm25 (ML)                         â”‚
â”‚                              - created_at                                  â”‚
â”‚                              - UNIQUE(city_id,                             â”‚
â”‚                                timestamp)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REST API (FastAPI / Python)                              â”‚
â”‚                           server.py                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GET /api/cities                                                            â”‚
â”‚    â†’ Returns all monitored cities from dim_city                             â”‚
â”‚    â†’ Used: UI city selector                                                â”‚
â”‚                                                                             â”‚
â”‚  GET /api/measurements?city_name={city_name}                                â”‚
â”‚    â†’ Query dm_dashboard_analytics WHERE city_name = ?                       â”‚
â”‚    â†’ Returns: last 168 rows (7 days Ã— 24 hours)                             â”‚
â”‚    â†’ Columns: city_name, timestamp, temperature, humidity, wind_speed,     â”‚
â”‚              pm10, pm25, predicted_pm25                                     â”‚
â”‚    â†’ Used: Dashboard charts                                                â”‚
â”‚                                                                             â”‚
â”‚  GET /                                                                      â”‚
â”‚    â†’ Health check: {"status": "EcoSense API is running"}                    â”‚
â”‚    â†’ Used: API availability test                                           â”‚
â”‚                                                                             â”‚
â”‚  CORS: Enabled for localhost:3000, localhost:5173, *                       â”‚
â”‚  Server: Uvicorn @ http://0.0.0.0:8000                                     â”‚
â”‚  Docs: Swagger @ http://localhost:8000/docs                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 CLIENT (TypeScript / React + Vite)                          â”‚
â”‚                         FÑ€Ğ¾Ğ½Ñ‚ĞµĞ½Ğ´                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Entry Point:             Core Components:        Data Flow:                â”‚
â”‚  â€¢ index.html            â€¢ App.tsx (440 lines)   â€¢ fetchCities()           â”‚
â”‚  â€¢ index.tsx             â€¢ DashboardCharts.tsx   â€¢ fetchDashboardData()    â”‚
â”‚  â€¢ vite.config.ts        â€¢ ETLLogs.tsx           â€¢ services/api.ts        â”‚
â”‚                          â€¢ types.ts              â€¢ Transform API response  â”‚
â”‚  Build: npm run build                                                       â”‚
â”‚  Dev: npm run dev (port 5173)                    Visualizations:          â”‚
â”‚  Type: React 19 + TypeScript                     â€¢ TrendsChart (PM2.5,     â”‚
â”‚  Styling: Tailwind CSS                             PM10, rolling avg,     â”‚
â”‚  Icons: Lucide React                               WHO threshold)          â”‚
â”‚  Charts: Recharts                                â€¢ CorrelationMatrix      â”‚
â”‚                                                   (Pearson correlation)    â”‚
â”‚                          Views:                   â€¢ TrueHeatmap (daily     â”‚
â”‚                          â€¢ Dashboard (KPI)         max PM2.5)              â”‚
â”‚                          â€¢ QA Report (DQ)        â€¢ ETLLogs (system logs)  â”‚
â”‚                          â€¢ Data Registry         â€¢ Data Registry (table)  â”‚
â”‚                          â€¢ Logs                  â€¢ Glossary (help)        â”‚
â”‚                          â€¢ Glossary (help)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ: Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸

### **1. Open-Meteo Weather API**
- **Endpoint**: `https://api.open-meteo.com/v1/forecast`
- **Parameters**:
  - `latitude`, `longitude`: Coordinates of city
  - `past_days=7`: Fetch 7 days of historical data
  - `forecast_days=1`: Fetch 1 day of forecast
  - `hourly=temperature_2m,relative_humidity_2m,wind_speed_10m`: Metrics
- **Response**: JSON with `hourly` array containing timestamps and measurements
- **Granularity**: Hourly
- **Cost**: Free, no authentication

### **2. Open-Meteo Air Quality API**
- **Endpoint**: `https://air-quality-api.open-meteo.com/v1/air-quality`
- **Parameters**:
  - `latitude`, `longitude`: Coordinates of city
  - `past_days=7`: Fetch 7 days historical
  - `forecast_days=1`: Fetch 1 day forecast
  - `hourly=pm10,pm2_5`: Air quality metrics
- **Response**: JSON with `hourly` array
- **Granularity**: Hourly
- **Cost**: Free, no authentication

### **3. Monitored Cities (13 cities)**

| City | Country | Coordinates | Use Case |
|------|---------|------------|----------|
| ĞœĞ¾ÑĞºĞ²Ğ° | RU | 55.7558, 37.6173 | Reference (Russia, capital) |
| Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³ | RU | 59.9343, 30.3351 | Northern Europe comparison |
| Ğ£Ğ»ÑŒÑĞ½Ğ¾Ğ²ÑĞº | RU | 54.3141, 48.4031 | Mid-Russia |
| ĞšĞ°Ğ·Ğ°Ğ½ÑŒ | RU | 55.7887, 49.1221 | Volga region |
| ĞĞ¾Ğ²Ğ¾ÑĞ¸Ğ±Ğ¸Ñ€ÑĞº | RU | 55.0084, 82.9357 | Siberia |
| Ğ•ĞºĞ°Ñ‚ĞµÑ€Ğ¸Ğ½Ğ±ÑƒÑ€Ğ³ | RU | 56.8389, 60.6057 | Ural region |
| Ğ›Ğ¾Ğ½Ğ´Ğ¾Ğ½ | GB | 51.5074, -0.1278 | Western Europe |
| Ğ‘ĞµÑ€Ğ»Ğ¸Ğ½ | DE | 52.5200, 13.4050 | Central Europe |
| ĞŸĞ°Ñ€Ğ¸Ğ¶ | FR | 48.8566, 2.3522 | Western Europe |
| Ğ Ğ¸Ğ¼ | IT | 41.9028, 12.4964 | Southern Europe |
| ĞŸĞµĞºĞ¸Ğ½ | CN | 39.9042, 116.4074 | Asian reference (high pollution) |
| Ğ¢Ğ¾ĞºĞ¸Ğ¾ | JP | 35.6762, 139.6503 | Japan (advanced pollution control) |
| Ğ”ÑƒĞ±Ğ°Ğ¹ | AE | 25.2048, 55.2708 | Middle East |

---

## ğŸ” Extract: Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

### **Function**: `fetch_open_meteo_data(lat, lng)`

Located in: `services/etl.py` (lines 47-59)

```python
def fetch_open_meteo_data(lat, lng):
    """Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¾Ğ·Ğ´ÑƒÑ…Ğ° Ğ·Ğ° 7 Ğ´Ğ½ĞµĞ¹"""
    print(f'Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ {lat}, {lng}...')
    
    # 1. ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ°
    w_url = f'https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lng}&past_days=7&forecast_days=1&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m'
    w_res = requests.get(w_url).json()
    
    # 2. Ğ’Ğ¾Ğ·Ğ´ÑƒÑ…
    aq_url = f'https://air-quality-api.open-meteo.com/v1/air-quality?latitude={lat}&longitude={lng}&past_days=7&forecast_days=1&hourly=pm10,pm2_5'
    aq_res = requests.get(aq_url).json()
    
    return w_res, aq_res
```

### **Parameters**:
- `lat`, `lng`: City coordinates (from CITIES list)

### **Returns**:
- `w_res`: Weather JSON
- `aq_res`: Air Quality JSON

### **Data Window**:
- Historical: 7 days
- Forecast: 1 day
- **Total**: 8 days Ã— 24 hours = 192 hourly records per city

### **Network Resilience**:
- No built-in retry logic (to be improved)
- Timeout: Default requests timeout
- Error handling: Wrapped in try-except in main()

### **Data Volume**:
- Per city: ~8 KB JSON
- 13 cities: ~104 KB per run
- Frequency: Manual execution (see Scheduling)

---

## ğŸ”„ Transform: ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

### **Functions Chain**:

#### **1. `transform_data(w_data, aq_data)`**
Located: `services/etl.py` (lines 62-88)

**Steps**:
1. Create weather DataFrame from `w_data['hourly']`
   - Columns: `time`, `temperature`, `humidity`, `wind_speed`
2. Create air quality DataFrame from `aq_data['hourly']`
   - Columns: `time`, `pm10`, `pm25`
3. Merge on `time` column (inner join)
   - Result: Single DataFrame with 8 columns
4. Convert `time` string to `timestamp` (ISO format â†’ datetime)
5. **Data Cleaning**:
   - Replace negative PM10 values with NULL (remove outliers)
   - Replace negative PM2.5 values with NULL
   - Rationale: API sometimes returns negative values for missing data

**Output**:
```python
DataFrame {
  time: str (ISO)
  timestamp: datetime
  temperature: float
  humidity: float
  wind_speed: float
  pm10: float (nullable)
  pm25: float (nullable)
}
```

#### **2. `train_and_predict_ml(df)`**
Located: `services/etl.py` (lines 91-122)

**Algorithm**: Linear Regression (sklearn)

**Target**: PM2.5 (predicted_pm25)

**Features**:
- `temperature` (Â°C): How temperature affects air composition
- `wind_speed` (km/h): Dispersion effect
- `humidity` (%): Moisture affects particle aggregation

**Training**:
1. Drop rows with NaN in [temperature, wind_speed, humidity, pm25]
2. Check minimum 10 samples requirement
3. Fit LinearRegression model
4. Log RÂ² score (model fit quality)

**Prediction**:
1. For rows where features are available (fill NaN with mean)
2. Generate `predicted_pm25` column
3. Round to 2 decimal places

**Why Linear Regression?**
- Interpretable weights (coefficient = feature importance)
- Fast training/prediction
- Suitable for hourly forecasting
- Captures linear relationships in weather-pollution

**Output**:
```python
DataFrame {
  ... (all previous columns)
  predicted_pm25: float (rounded to 2 decimals)
}
```

### **Data Aggregation**:
No aggregation at transform stage. Maintains hourly granularity.

---

## âœ… Validate (DQ): ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°

### **Validation Points**:

Located: `services/etl.py` (implied in main() flow) + `App.tsx` (frontend QA)

#### **1. In ETL (Backend)**:

| Check | Logic | Action |
|-------|-------|--------|
| **Negative PM values** | `if pm10 < 0 or pm25 < 0` | Replace with NULL |
| **Insufficient data for ML** | `if len(train_df) < 10` | Log warning, set predicted_pm25 = NULL |
| **Data merge completeness** | Inner join weather + air | May lose rows if timestamps don't align |

#### **2. In Frontend (App.tsx)**:

**QA Report View** calculates:

```typescript
// 1. Missing data count
const missingPM = data.filter(d => d.pm25 === null).length;
const missingPct = ((missingPM / data.length) * 100).toFixed(1);

// 2. Quality Score
const qualityScore = Math.max(0, 100 - (missingPM / data.length * 100));

// 3. Timestamp monotonicity check
let timestampIssues = 0;
for (let i = 1; i < data.length; i++) {
  const prev = new Date(data[i - 1].timestamp).getTime();
  const curr = new Date(data[i].timestamp).getTime();
  if (curr <= prev) timestampIssues++;
}

// 4. Anomaly detection (spikes > 50%)
let anomalyCount = 0;
for (let i = 1; i < data.length; i++) {
  const change = Math.abs((curr - prev) / prev);
  if (change > 0.5) anomalyCount++;
}

// 5. WHO Threshold Exceedance
const WHO_LIMIT_DAILY = 15 // Âµg/mÂ³
const daysExceedingWHO = Array.from(dailyMax.values())
  .filter(v => v > WHO_LIMIT_DAILY).length;
```

**QA Metrics Displayed**:
- Total rows: Count of records
- Missing PM2.5: Count + percentage
- Quality Score: 0-100 based on completeness
- Timestamp issues: Monotonicity violations
- Anomalies: Sudden spikes
- Days exceeding WHO: Health risk indicator

### **Quality Standards**:
- **Good**: Quality Score > 95%, Anomalies < 2, Timestamp issues = 0
- **Fair**: Quality Score 80-95%, Anomalies 2-5
- **Poor**: Quality Score < 80%, Manual investigation needed

---

## ğŸ“¥ Load: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ² Ğ‘Ğ”

### **Function**: `load_to_db(conn, df, city_name, lat, lng, country)`
Located: `services/etl.py` (lines 125-180)

### **Step 1: Upsert City Dimension**

```sql
INSERT INTO dim_city (name, country_code, latitude, longitude)
VALUES (%s, %s, %s, %s)
ON CONFLICT (name) DO NOTHING
RETURNING city_id;
```

**Logic**:
- If city exists: ignore (do nothing)
- If city is new: insert and return city_id
- Fallback: Query city_id from table if insert didn't return

**Table**: `dim_city`
- **city_id** (SERIAL PRIMARY KEY)
- **name** (VARCHAR 100, UNIQUE)
- **country_code** (VARCHAR 5)
- **latitude** (DECIMAL 9,6)
- **longitude** (DECIMAL 9,6)

### **Step 2: Prepare Data**

Convert DataFrame rows to SQL tuples:

```python
for _, row in df.iterrows():
  ts = row['timestamp']
  weather_data.append((city_id, ts, row['temperature'], row['humidity'], row['wind_speed']))
  aq_data.append((city_id, ts, row['pm10'], row['pm25'], row['predicted_pm25']))
```

### **Step 3: Batch Insert Weather**

```sql
INSERT INTO fact_weather (city_id, timestamp, temperature, humidity, wind_speed)
VALUES %s
ON CONFLICT (city_id, timestamp) DO UPDATE
SET temperature=EXCLUDED.temperature, 
    humidity=EXCLUDED.humidity, 
    wind_speed=EXCLUDED.wind_speed;
```

**Table**: `fact_weather`
- **weather_id** (SERIAL PRIMARY KEY)
- **city_id** (INTEGER FK â†’ dim_city)
- **timestamp** (TIMESTAMP)
- **temperature** (DECIMAL 5,2)
- **humidity** (DECIMAL 5,2)
- **wind_speed** (DECIMAL 5,2)
- **created_at** (TIMESTAMP DEFAULT CURRENT_TIMESTAMP)
- **UNIQUE(city_id, timestamp)** â€” Prevents duplicates

### **Step 4: Batch Insert Air Quality**

```sql
INSERT INTO fact_air_quality (city_id, timestamp, pm10, pm25, predicted_pm25)
VALUES %s
ON CONFLICT (city_id, timestamp) DO UPDATE
SET pm10=EXCLUDED.pm10, 
    pm25=EXCLUDED.pm25, 
    predicted_pm25=EXCLUDED.predicted_pm25;
```

**Table**: `fact_air_quality`
- **aq_id** (SERIAL PRIMARY KEY)
- **city_id** (INTEGER FK â†’ dim_city)
- **timestamp** (TIMESTAMP)
- **pm10** (DECIMAL 6,2)
- **pm25** (DECIMAL 6,2)
- **predicted_pm25** (DECIMAL 6,2)
- **created_at** (TIMESTAMP DEFAULT CURRENT_TIMESTAMP)
- **UNIQUE(city_id, timestamp)** â€” Prevents duplicates

### **Batch Insert Library**:
- Uses `psycopg2.extras.execute_values()`
- More efficient than individual INSERT statements
- Single statement for all rows
- For 13 cities Ã— 192 hours = 2,496 rows per run

### **Conflict Resolution**:
- Strategy: **Upsert** (Update if exists, Insert if new)
- Rationale: Handles re-runs, data corrections, forecast updates
- Same timestamp + city = overwrite with new data

### **Indexes**:

```sql
CREATE INDEX idx_weather_time ON fact_weather(timestamp);
CREATE INDEX idx_aq_time ON fact_air_quality(timestamp);
```

- Optimize: Filter by timestamp range (API query)
- Optimize: Dashboard query for last 168 rows

---

## ğŸ’¾ Storage: Ğ’Ğ¸Ñ‚Ñ€Ğ¸Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

### **SQL VIEW**: `dm_dashboard_analytics`

**Purpose**: Provide single query entry point for frontend

**Definition** (from `ecosense.sql`):

```sql
CREATE OR REPLACE VIEW dm_dashboard_analytics AS
SELECT 
    c.name as city_name,
    w.timestamp,
    w.temperature,
    w.humidity,
    w.wind_speed,
    aq.pm10,
    aq.pm25,
    aq.predicted_pm25
FROM fact_weather w
JOIN dim_city c ON w.city_id = c.city_id
LEFT JOIN fact_air_quality aq ON w.city_id = aq.city_id AND w.timestamp = aq.timestamp
ORDER BY w.timestamp DESC;
```

**Join Strategy**:
1. INNER JOIN: `fact_weather` + `dim_city`
   - All weather rows with city name
2. LEFT JOIN: Result + `fact_air_quality`
   - Preserve weather rows even if AQ data is missing
   - Can handle partial data (e.g., weather without AQ)

**Columns** (8 total):
| Column | Source | Type | Nullable |
|--------|--------|------|----------|
| city_name | dim_city.name | VARCHAR | NO |
| timestamp | fact_weather.timestamp | TIMESTAMP | NO |
| temperature | fact_weather.temperature | DECIMAL | YES |
| humidity | fact_weather.humidity | DECIMAL | YES |
| wind_speed | fact_weather.wind_speed | DECIMAL | YES |
| pm10 | fact_air_quality.pm10 | DECIMAL | YES |
| pm25 | fact_air_quality.pm25 | DECIMAL | YES |
| predicted_pm25 | fact_air_quality.predicted_pm25 | DECIMAL | YES |

**Ordering**: Descending by timestamp (newest first)

**Query Cost**:
- Index usage: `idx_weather_time`, `idx_aq_time`
- Full view scan: ~168 rows per city per request
- Typical response time: < 100ms

---

## ğŸ”Œ API: REST endpoints

### **Server**: FastAPI (Python)
**File**: `server.py`
**Host**: `0.0.0.0`
**Port**: `8000`
**Docs**: `http://localhost:8000/docs` (Swagger UI)

### **Endpoints**:

#### **1. GET /api/cities**

**Purpose**: Fetch list of monitored cities

**Response**:
```json
{
  "cities": [
    {
      "city_id": 1,
      "name": "ĞœĞ¾ÑĞºĞ²Ğ°",
      "country_code": "RU",
      "latitude": 55.7558,
      "longitude": 37.6173
    },
    ...
  ]
}
```

**Query**:
```sql
SELECT * FROM dim_city ORDER BY name
```

**Error Handling**:
- 500: Database connection error
  - Message: str(exception)

**Used by**: Frontend city selector dropdown

---

#### **2. GET /api/measurements**

**Purpose**: Fetch dashboard data for a specific city

**Query Parameters**:
- `city_name` (required): City name (string)
  - Example: `?city_name=ĞœĞ¾ÑĞºĞ²Ğ°`

**Response**:
```json
{
  "data": [
    {
      "city_name": "ĞœĞ¾ÑĞºĞ²Ğ°",
      "timestamp": "2025-12-25T12:00:00",
      "temperature": 5.2,
      "humidity": 72.5,
      "wind_speed": 8.3,
      "pm10": 45.2,
      "pm25": 18.7,
      "predicted_pm25": 19.3
    },
    ...
  ]
}
```

**Query**:
```sql
SELECT * FROM dm_dashboard_analytics
WHERE city_name = %s
ORDER BY timestamp DESC
LIMIT 168
```

**Limit**: 168 rows (7 days Ã— 24 hours)

**Error Handling**:
- 500: Database error
  - Message: "Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ etl.py"

**Used by**: Dashboard charts, data table

---

#### **3. GET /**

**Purpose**: Health check

**Response**:
```json
{
  "status": "EcoSense API is running"
}
```

**Used by**: Infrastructure monitoring, API availability test

---

### **Middleware**: CORS

```python
origins = [
    "http://localhost:3000",
    "http://localhost:5173",
    "*"  # For testing
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**Rationale**:
- localhost:3000: React dev server (alternative)
- localhost:5173: Vite dev server (current)
- *: Wildcard for testing

**Note**: Wildcard should be restricted in production

---

### **Data Flow**:

```
Client Request
    â†“
FastAPI route handler
    â†“
get_db_connection() â†’ psycopg2.connect()
    â†“
Execute SQL query on VIEW/TABLE
    â†“
Cursor.fetchall() â†’ RealDictCursor (returns dicts)
    â†“
Format response JSON
    â†“
Return to client
    â†“
Close cursor + connection
```

---

## ğŸ–¥ï¸ UI: Ğ¤Ñ€Ğ¾Ğ½Ñ‚ĞµĞ½Ğ´

### **Framework**: React 19 + TypeScript + Vite

**Entry Point**: `index.html` â†’ `index.tsx` â†’ `App.tsx`

**Build**:
```bash
npm install       # Install dependencies
npm run dev       # Development: http://localhost:5173
npm run build     # Production bundle
npm run preview   # Preview production build
```

### **Main Component**: `App.tsx` (440 lines)

**State**:
```typescript
const [activeView, setActiveView] = useState<'dashboard' | 'qa' | 'data' | 'logs' | 'glossary'>('dashboard');
const [cities, setCities] = useState<City[]>([]);
const [selectedCityName, setSelectedCityName] = useState<string>('');
const [data, setData] = useState<UnifiedDataPoint[]>([]);
const [loading, setLoading] = useState(false);
const [logs, setLogs] = useState<LogEntry[]>([]);
const [showMLForecast, setShowMLForecast] = useState(true);
const [chartMode, setChartMode] = useState<'simple' | 'detailed' | 'weather'>('simple');
```

**Lifecycle**:
1. On mount: `fetchCities()` â†’ populate city selector
2. On city change: `loadData()` â†’ `fetchDashboardData(cityName)`
3. Transform API response â†’ `UnifiedDataPoint[]`
4. Render selected view

### **Views**:

#### **1. Dashboard** ğŸ¯

**Components**:
- **KPI Cards**:
  - Current PM2.5 (last value)
  - Current Temperature
  - Current Humidity
  - Days exceeding WHO threshold
  - Quality Score (%)
  - Missing data count

- **Charts** (Recharts):
  - **TrendsChart**: 
    - PM2.5 (area)
    - PM10 (optional)
    - Rolling average (6-hour)
    - WHO threshold line (15 Âµg/mÂ³)
    - AI prediction (dashed line)
    - Features: `simple | detailed | weather` modes
    
  - **CorrelationMatrix**:
    - 4Ã—4 matrix (PM2.5, Temperature, Wind, Humidity)
    - Pearson correlation coefficients
    - Color-coded: red (positive) to blue (negative)
    
  - **TrueHeatmap**:
    - Daily max PM2.5 by day
    - Heatmap colors: green (good) to red (bad)

**Calculations**:
```typescript
// stats object
const stats = useMemo(() => {
  // 1. Days exceeding WHO
  const dailyMax = new Map<string, number>();
  data.forEach(d => { /* calculate daily max */ });
  const daysExceedingWHO = Array.from(dailyMax.values())
    .filter(v => v > WHO_LIMIT_DAILY).length;
  
  // 2. Missing data
  const missingPM = data.filter(d => d.pm25 === null).length;
  const qualityScore = Math.max(0, 100 - (missingPM / data.length * 100));
  
  // 3. Timestamp issues
  let timestampIssues = 0;
  for (let i = 1; i < data.length; i++) {
    if (curr_ts <= prev_ts) timestampIssues++;
  }
  
  // 4. Anomalies
  let anomalyCount = 0;
  for (let i = 1; i < data.length; i++) {
    if (change > 0.5) anomalyCount++;
  }
  
  return { current, daysExceedingWHO, ..., anomalyCount };
}, [data]);
```

#### **2. QA Report** ğŸ“‹

**Purpose**: Data quality assessment

**Displays**:
- Total rows
- Missing PM2.5 count & percentage
- Quality Score (%)
- Timestamp monotonicity issues
- Anomaly count (spikes > 50%)
- Days exceeding WHO limit
- ML model RÂ² (if available)

**Interpretation**:
- Green: Score > 95%
- Yellow: 80-95%
- Red: < 80%

#### **3. Data Registry** ğŸ“Š

**Purpose**: Browse raw measurements

**Columns**:
- Timestamp
- PM2.5, PM10
- Temperature, Humidity, Wind Speed
- Predicted PM2.5
- Freshness indicator

**Features**:
- Sortable table
- Scrollable (last 168 rows)
- Shows data freshness

#### **4. Logs** ğŸ“

**Purpose**: System operation history

**Log Types**:
- `info`: General messages
- `success`: Operation completed
- `error`: Error occurred
- `warning`: Data quality issue

**Format**: `[HH:MM:SS] TYPE: Message`

**Events Logged**:
- ETL run start/completion
- API errors
- Data load success/failure
- City selection
- QA validation results

#### **5. Glossary** â„¹ï¸

**Purpose**: Help documentation

**Contents**:
- Metric definitions (PM2.5, PM10, WHO standards)
- API explanation
- ETL pipeline overview
- Data sources
- Links to documentation

---

### **API Client**: `services/api.ts`

```typescript
const BACKEND_URL = 'http://localhost:8000/api';

export const fetchCities = async (): Promise<City[]> => {
  const response = await fetch(`${BACKEND_URL}/cities`);
  if (!response.ok) throw new Error('ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ñ ÑĞµÑ€Ğ²ĞµÑ€Ğ°');
  const data = await response.json();
  return data.cities.map((c: any) => ({
    name: c.name,
    lat: c.latitude,
    lng: c.longitude,
    country: c.country_code
  }));
};

export const fetchDashboardData = async (cityName: string): Promise<UnifiedDataPoint[]> => {
  const response = await fetch(`${BACKEND_URL}/measurements?city_name=${encodeURIComponent(cityName)}`);
  if (!response.ok) throw new Error(`ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°: ${response.statusText}`);
  const json = await response.json();
  
  return json.data.map((row: any) => ({
    timestamp: row.timestamp,
    displayTime: new Date(row.timestamp).toLocaleDateString('ru-RU', {
      month: 'short', day: 'numeric', hour: '2-digit', minute: '2-digit'
    }),
    pm10: row.pm10,
    pm25: row.pm25,
    temperature: row.temperature,
    windSpeed: row.wind_speed,
    humidity: row.humidity,
    predictedPM25: row.predicted_pm25
  }));
};
```

---

## â° Scheduling & Monitoring

### **Current State**: Manual execution

```bash
python services/etl.py
```

**When to run**:
- After database initialization
- On-demand data refresh
- Development testing

### **Recommended**: Scheduled execution

**Option 1: Cron Job** (Linux/macOS)
```bash
# Run every hour
0 * * * * cd /path/to/project && python services/etl.py

# Run every 6 hours
0 */6 * * * cd /path/to/project && python services/etl.py
```

**Option 2: Celery** (Python task queue)
```python
from celery import Celery
from celery.schedules import crontab

app = Celery('ecosense')
app.conf.beat_schedule = {
    'run-etl-every-6-hours': {
        'task': 'services.etl.main',
        'schedule': crontab(minute=0, hour='*/6'),
    },
}
```

**Option 3: APScheduler** (Python)
```python
from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()
scheduler.add_job(main, 'interval', hours=6)
scheduler.start()
```

**Option 4: Docker + Kubernetes**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ecosense-etl
spec:
  schedule: "0 */6 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: etl
            image: ecosense:latest
            command: ["python", "services/etl.py"]
          restartPolicy: OnFailure
```

### **Recommended Frequency**: Every 6 hours
- Captures data variations
- Reduces API calls vs. hourly
- Provides adequate freshness for analytics

### **Monitoring**:

**Metrics to Track**:
1. **Last run time**: When ETL last completed
2. **Run duration**: How long the ETL takes
3. **Data freshness**: Max(timestamp in DB) - now()
4. **Success rate**: Successful runs / total runs
5. **Errors**: Failed city extractions, DB connection issues
6. **Data quality**: Quality score trend

**Logging**:

Current (basic):
```python
print(f'Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ {lat}, {lng}...')
print(f'Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ {city_name} ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² Ğ‘Ğ”.')
print("ETL ĞŸĞ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
```

**Recommended** (structured):
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('etl.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Usage
logger.info(f"ETL started for {city['name']}")
logger.error(f"Failed to fetch {city['name']}: {str(e)}")
logger.info(f"ETL completed in {elapsed_time}s")
```

---

## âš ï¸ Error Handling & Retry Logic

### **Current Implementation**: Minimal

```python
def main():
    conn = get_db_connection()
    if not conn:
        return

    for city in CITIES:
        try:
            # ETL Pipeline
            w, aq = fetch_open_meteo_data(city['lat'], city['lng'])
            df = transform_data(w, aq)
            df = train_and_predict_ml(df)
            load_to_db(conn, df, city['name'], city['lat'], city['lng'], city['country'])
        except Exception as e:
            print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ° {city['name']}: {e}")

    conn.close()
```

**Issues**:
- No retry logic on API failures
- Single exception catches all errors
- No rollback on partial failures
- No timeout handling

### **Recommended Improvements**:

#### **1. API Call Retry (Exponential Backoff)**

```python
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def create_session():
    session = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=0.5,  # 0.5s, 1s, 2s
        status_forcelist=[429, 500, 502, 503, 504]
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

def fetch_open_meteo_data(lat, lng):
    session = create_session()
    try:
        w_res = session.get(w_url, timeout=10).json()
        aq_res = session.get(aq_url, timeout=10).json()
        return w_res, aq_res
    except requests.Timeout:
        logger.error(f"Timeout for {lat},{lng}")
        raise
    except requests.RequestException as e:
        logger.error(f"API error: {e}")
        raise
```

#### **2. Database Connection Retry**

```python
def get_db_connection(max_retries=3):
    for attempt in range(max_retries):
        try:
            conn = psycopg2.connect(**DB_CONFIG)
            return conn
        except psycopg2.OperationalError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # 1s, 2s, 4s
                logger.warning(f"DB connection failed, retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                logger.error(f"Failed to connect to DB after {max_retries} attempts")
                raise
```

#### **3. Transaction Rollback on Error**

```python
def load_to_db(conn, df, city_name, lat, lng, country):
    try:
        cur = conn.cursor()
        
        # Upsert city
        cur.execute(...)
        city_id = ...
        
        # Batch inserts
        execute_values(cur, insert_weather_query, weather_data)
        execute_values(cur, insert_aq_query, aq_data)
        
        conn.commit()
        logger.info(f"Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ {city_name} ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹")
        
    except Exception as e:
        conn.rollback()
        logger.error(f"Error loading {city_name}: {e}")
        raise
    finally:
        cur.close()
```

#### **4. Partial Failure Handling**

```python
def main():
    conn = get_db_connection()
    failed_cities = []
    
    for city in CITIES:
        try:
            w, aq = fetch_open_meteo_data(city['lat'], city['lng'])
            df = transform_data(w, aq)
            df = train_and_predict_ml(df)
            load_to_db(conn, df, city['name'], city['lat'], city['lng'], city['country'])
            logger.info(f"âœ“ {city['name']}")
            
        except Exception as e:
            logger.error(f"âœ— {city['name']}: {e}")
            failed_cities.append(city['name'])
    
    conn.close()
    
    if failed_cities:
        logger.warning(f"Failed cities: {', '.join(failed_cities)}")
        # Send alert/email
    else:
        logger.info("âœ“ All cities processed successfully")
```

#### **5. Data Validation Before Load**

```python
def validate_data(df):
    """Validation before database insert"""
    issues = []
    
    # Check timestamp range
    if df['timestamp'].isna().any():
        issues.append("Null timestamps found")
    
    # Check value ranges
    if (df['temperature'] < -50).any() or (df['temperature'] > 60).any():
        issues.append("Temperature out of realistic range")
    
    if (df['pm25'] < 0).any():
        issues.append("Negative PM2.5 values (should be filtered)")
    
    if df.shape[0] < 100:
        issues.append(f"Only {df.shape[0]} rows (expected ~192)")
    
    if issues:
        raise ValueError("Data validation failed: " + "; ".join(issues))
    
    return True
```

---

## ğŸ“‹ Summary Table: Complete Pipeline

| Stage | Component | Technology | Input | Output | Frequency | Error Handling |
|-------|-----------|-----------|-------|--------|-----------|-----------------|
| **Extract** | fetch_open_meteo_data | requests | City coords | JSON (weather, AQ) | Manual / 6h | Generic try-catch |
| **Transform** | transform_data + train_and_predict_ml | pandas, sklearn | JSON | DataFrame + ML predictions | Inline (per batch) | NaN removal |
| **Validate** | Implicit checks | Python logic | DataFrame | Warnings/logs | Inline | Log only |
| **Load** | load_to_db | psycopg2 + execute_values | DataFrame | Rows in DB | Inline | Rollback on error |
| **Storage** | PostgreSQL + VIEW | pg | Fact/Dim tables | dm_dashboard_analytics | Persistent | Constraints + indexes |
| **API** | FastAPI server | Python | HTTP requests | JSON responses | Real-time | 500 error responses |
| **UI** | React + Recharts | TypeScript | /api/cities, /api/measurements | Interactive charts | On demand | User-facing error messages |

---

## ğŸš€ Deployment Checklist

- [ ] PostgreSQL configured with correct tables (ecosense.sql applied)
- [ ] Python virtual environment created
- [ ] `requirements.txt` installed
- [ ] ETL run once successfully (`python services/etl.py`)
- [ ] FastAPI server starts (`python server.py`)
- [ ] Frontend dependencies installed (`npm install`)
- [ ] Frontend dev server starts (`npm run dev`)
- [ ] Health check passes (`GET http://localhost:8000/`)
- [ ] Cities endpoint works (`GET http://localhost:8000/api/cities`)
- [ ] Dashboard loads data for a city (`GET http://localhost:8000/api/measurements?city_name=ĞœĞ¾ÑĞºĞ²Ğ°`)
- [ ] React UI displays charts without errors
- [ ] Scheduled job configured (cron/Celery/APScheduler)
- [ ] Logs directory configured
- [ ] Database backups scheduled

---

## ğŸ“š References

- **Open-Meteo API**: https://open-meteo.com/
- **WHO Air Quality Standards**: https://www.who.int/publications/i/item/9789240034228
- **FastAPI**: https://fastapi.tiangolo.com/
- **React**: https://react.dev/
- **PostgreSQL**: https://www.postgresql.org/
- **Recharts**: https://recharts.org/
- **Scikit-learn**: https://scikit-learn.org/

---

**Document Version**: 1.0  
**Last Updated**: 2025-12-25  
**Maintained By**: EcoSense Team
